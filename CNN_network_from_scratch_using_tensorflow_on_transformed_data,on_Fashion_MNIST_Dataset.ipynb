{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagargomad/My_projects/blob/main/CNN_network_from_scratch_using_tensorflow_on_transformed_data%2Con_Fashion_MNIST_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMc78-T4VTi3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "FvT-FbbUjBKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjLtpHsRVa_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image):\n",
        "    # Resize image to a quadratic shape (e.g., 64x64) using OpenCV\n",
        "    image = cv2.resize(image, (64, 64))\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "# Apply preprocessing to all datasets\n",
        "train_images = np.array([preprocess_image(img) for img in train_images])\n",
        "val_images = np.array([preprocess_image(img) for img in val_images])\n",
        "test_images = np.array([preprocess_image(img) for img in test_images])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKk1eYcsjBJi",
        "outputId": "5e4469f4-d96c-4081-ed13-76746f3d8fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBTmPxP5Va54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images[..., np.newaxis], train_labels, epochs=10, validation_data=(val_images[..., np.newaxis], val_labels))\n",
        "\n",
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(model.history.history['val_accuracy'])\n",
        "\n",
        "# Save the model with the highest validation accuracy\n",
        "model.save(f'fashion_mnist_cnn_best_model_epoch_{best_epoch}.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7GxHSFEjBFU",
        "outputId": "5dbe5bbd-9820-46a9-dcfb-45c1e029c4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 243s 161ms/step - loss: 0.4197 - accuracy: 0.8504 - val_loss: 0.3181 - val_accuracy: 0.8839\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 242s 162ms/step - loss: 0.2742 - accuracy: 0.8994 - val_loss: 0.2687 - val_accuracy: 0.9007\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 237s 158ms/step - loss: 0.2257 - accuracy: 0.9154 - val_loss: 0.2689 - val_accuracy: 0.9010\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 248s 165ms/step - loss: 0.1862 - accuracy: 0.9312 - val_loss: 0.2421 - val_accuracy: 0.9106\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 236s 158ms/step - loss: 0.1538 - accuracy: 0.9421 - val_loss: 0.2486 - val_accuracy: 0.9153\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 239s 159ms/step - loss: 0.1281 - accuracy: 0.9522 - val_loss: 0.2425 - val_accuracy: 0.9159\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 240s 160ms/step - loss: 0.1009 - accuracy: 0.9619 - val_loss: 0.2627 - val_accuracy: 0.9182\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 238s 159ms/step - loss: 0.0812 - accuracy: 0.9699 - val_loss: 0.3084 - val_accuracy: 0.9105\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 235s 157ms/step - loss: 0.0671 - accuracy: 0.9758 - val_loss: 0.3069 - val_accuracy: 0.9186\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 234s 156ms/step - loss: 0.0530 - accuracy: 0.9801 - val_loss: 0.3591 - val_accuracy: 0.9137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n71i0LllVa5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to preprocess and augment images\n",
        "def preprocess_and_augment(image):\n",
        "    # Resize image to a quadratic shape (e.g., 64x64) using OpenCV\n",
        "    image = cv2.resize(image, (64, 64))\n",
        "    # Add an extra channel to make it compatible with ImageDataGenerator\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "# Define data augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Apply preprocessing and augmentation to all datasets\n",
        "train_images = np.array([preprocess_and_augment(img) for img in train_images])\n",
        "val_images = np.array([preprocess_and_augment(img) for img in val_images])\n",
        "test_images = np.array([preprocess_and_augment(img) for img in test_images])\n",
        "\n",
        "# Define and train the CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n",
        "\n",
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(model.history.history['val_accuracy'])\n",
        "\n",
        "# Save the model with the highest validation accuracy\n",
        "model.save(f'fashion_mnist_cnn_best_model_epoch_{best_epoch}.h5')\n",
        "\n",
        "# Use the saved model for inference\n",
        "loaded_model = tf.keras.models.load_model(f'fashion_mnist_cnn_best_model_epoch_{best_epoch}.h5')\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions = loaded_model.predict(test_images)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = loaded_model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWkyHdVZjA68",
        "outputId": "e3691a28-9875-40e5-e674-c4b53fb70eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 248s 165ms/step - loss: 0.4167 - accuracy: 0.8511 - val_loss: 0.3169 - val_accuracy: 0.8827\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 242s 161ms/step - loss: 0.2773 - accuracy: 0.8995 - val_loss: 0.2714 - val_accuracy: 0.8988\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 243s 162ms/step - loss: 0.2276 - accuracy: 0.9161 - val_loss: 0.2532 - val_accuracy: 0.9097\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 222s 148ms/step - loss: 0.1913 - accuracy: 0.9282 - val_loss: 0.2400 - val_accuracy: 0.9132\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 223s 149ms/step - loss: 0.1559 - accuracy: 0.9415 - val_loss: 0.2468 - val_accuracy: 0.9160\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 224s 150ms/step - loss: 0.1331 - accuracy: 0.9508 - val_loss: 0.2539 - val_accuracy: 0.9124\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 232s 155ms/step - loss: 0.1080 - accuracy: 0.9602 - val_loss: 0.2781 - val_accuracy: 0.9093\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 228s 152ms/step - loss: 0.0879 - accuracy: 0.9666 - val_loss: 0.2845 - val_accuracy: 0.9151\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 236s 157ms/step - loss: 0.0722 - accuracy: 0.9738 - val_loss: 0.3084 - val_accuracy: 0.9131\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 235s 157ms/step - loss: 0.0590 - accuracy: 0.9776 - val_loss: 0.3578 - val_accuracy: 0.9167\n",
            "313/313 [==============================] - 13s 40ms/step\n",
            "313/313 [==============================] - 13s 40ms/step - loss: 0.3958 - accuracy: 0.9153\n",
            "Test accuracy: 91.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSp5cXZsVa01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to preprocess and augment images\n",
        "def preprocess_and_augment(image):\n",
        "    # Resize image to a quadratic shape (e.g., 64x64) using OpenCV\n",
        "    image = cv2.resize(image, (64, 64))\n",
        "    # Add an extra channel to make it compatible with ImageDataGenerator\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "# Define data augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Apply preprocessing and augmentation to all datasets\n",
        "train_images = np.array([preprocess_and_augment(img) for img in train_images])\n",
        "val_images = np.array([preprocess_and_augment(img) for img in val_images])\n",
        "test_images = np.array([preprocess_and_augment(img) for img in test_images])\n",
        "\n",
        "# Define and train the CNN model with Batch Normalization and Dropout\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout layer with a dropout rate of 0.5\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks for ReduceLRonPlateau and Early Stopping\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels), callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "# Use the saved model for inference\n",
        "loaded_model = model\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions = loaded_model.predict(test_images)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = loaded_model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpAL5G4ojA1E",
        "outputId": "671f0433-7425-48b0-9568-46ed1e3bbcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 338s 222ms/step - loss: 0.8869 - accuracy: 0.6841 - val_loss: 0.4738 - val_accuracy: 0.8224 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 332s 221ms/step - loss: 0.6044 - accuracy: 0.7672 - val_loss: 0.3859 - val_accuracy: 0.8721 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 338s 225ms/step - loss: 0.5135 - accuracy: 0.8028 - val_loss: 0.3414 - val_accuracy: 0.8737 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 332s 221ms/step - loss: 0.4589 - accuracy: 0.8251 - val_loss: 0.3427 - val_accuracy: 0.8876 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 328s 219ms/step - loss: 0.4217 - accuracy: 0.8391 - val_loss: 0.3239 - val_accuracy: 0.8957 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 323s 215ms/step - loss: 0.3945 - accuracy: 0.8506 - val_loss: 0.2979 - val_accuracy: 0.8973 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 319s 212ms/step - loss: 0.3658 - accuracy: 0.8647 - val_loss: 0.2826 - val_accuracy: 0.9040 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 320s 213ms/step - loss: 0.3424 - accuracy: 0.8719 - val_loss: 0.2739 - val_accuracy: 0.9071 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 317s 212ms/step - loss: 0.3219 - accuracy: 0.8799 - val_loss: 0.2757 - val_accuracy: 0.9022 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 315s 210ms/step - loss: 0.3038 - accuracy: 0.8892 - val_loss: 0.2975 - val_accuracy: 0.9019 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 320s 213ms/step - loss: 0.2843 - accuracy: 0.8960 - val_loss: 0.2659 - val_accuracy: 0.9138 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 319s 213ms/step - loss: 0.2686 - accuracy: 0.8993 - val_loss: 0.2481 - val_accuracy: 0.9150 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 314s 209ms/step - loss: 0.2536 - accuracy: 0.9078 - val_loss: 0.2536 - val_accuracy: 0.9118 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 325s 216ms/step - loss: 0.2358 - accuracy: 0.9136 - val_loss: 0.2809 - val_accuracy: 0.9132 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 322s 215ms/step - loss: 0.2233 - accuracy: 0.9187 - val_loss: 0.3002 - val_accuracy: 0.8957 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 318s 212ms/step - loss: 0.1908 - accuracy: 0.9299 - val_loss: 0.2398 - val_accuracy: 0.9215 - lr: 2.0000e-04\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 319s 213ms/step - loss: 0.1743 - accuracy: 0.9349 - val_loss: 0.2445 - val_accuracy: 0.9230 - lr: 2.0000e-04\n",
            "Epoch 18/50\n",
            "1500/1500 [==============================] - 318s 212ms/step - loss: 0.1646 - accuracy: 0.9389 - val_loss: 0.2444 - val_accuracy: 0.9235 - lr: 2.0000e-04\n",
            "Epoch 19/50\n",
            "1500/1500 [==============================] - 319s 213ms/step - loss: 0.1595 - accuracy: 0.9412 - val_loss: 0.2533 - val_accuracy: 0.9213 - lr: 2.0000e-04\n",
            "Epoch 20/50\n",
            "1500/1500 [==============================] - 320s 213ms/step - loss: 0.1504 - accuracy: 0.9447 - val_loss: 0.2509 - val_accuracy: 0.9238 - lr: 4.0000e-05\n",
            "Epoch 21/50\n",
            "1500/1500 [==============================] - 319s 212ms/step - loss: 0.1450 - accuracy: 0.9461 - val_loss: 0.2573 - val_accuracy: 0.9241 - lr: 4.0000e-05\n",
            "313/313 [==============================] - 16s 50ms/step\n",
            "313/313 [==============================] - 16s 50ms/step - loss: 0.2609 - accuracy: 0.9220\n",
            "Test accuracy: 92.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFT7nXbsVaz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Te7q22ZVaux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nhjq_GejVat4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}